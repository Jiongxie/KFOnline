## 统一接入层

切换root账号：su root    密码：TU2Vx$8Mt**W8G4GzBPo
nginx.conf：  /export/servers/jfe/conf  
lua脚本：     /export/servers/security  
脚本上传：    rz

## 命令汇总

> ###机器配置查看

- 查看cpu个数

        总核数 = 物理CPU个数 * 每颗物理CPU的核数
        总逻辑CPU数 = 物理CPU个数 * 每颗物理CPU的核数 * 超线程数

        查看物理CPU个数
        cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l

        查看每个物理CPU中core的个数(即核数)
        cat /proc/cpuinfo |grep "cpu cores" |uniq

        查看逻辑CPU的个数
        cat /proc/cpuinfo| grep "processor"| wc -l

- 查看CPU信息（型号）

        cat /proc/cpuinfo |grep name |cut -fs -d :|uniq -c

- 查看内存信息

        cat /proc/meminfo
        
- 查询请求丢包命令

        mtr www.baidu.com        
        

> ### 谢继永-验证后的统计命令
     
     1、查询第一行日志
        awk 'NR ==1 {print}' access.log
     2、统计访问量
        cat access.log | wc -l
     3、统计域名的访问量
        cat access.log | awk -F '|' '{print $5}'|sort|uniq -c|sort -rn|head -5
           





> # logRotate.sh 脚本  
    
    #!/bin/bash
    
    # 定义日志文件路径
    NGINX_LOG_DIR=/export/servers/logs
    
    # 定义nginx 路径
    NGINX_PATH=/export/servers/jfe
    NGINX_PID_PATH=/export/servers/jfe/nginx.pid
    
    # 定义脚本执行日志文件
    outputLogFile=${NGINX_LOG_DIR}/logrotate.log
    
    # 主函数入口
    start(){
      echo `date '+%Y-%m-%d %H:%M:%S'`'- 执行nginx日志文件归档开始...'>>${outputLogFile}
      # 归档当前日志
      executeLogRotate
    
      # 切换nginx日志输出流
      executeReloadOutput $?
    
      # 清除历史归档文件
      executeClean $1 
    
      echo `date '+%Y-%m-%d %H:%M:%S'`'- 执行nginx日志文件归档结束！'>>${outputLogFile}
    }
    
    # 归档当前日志文件，归档为昨天的日志文件
    executeLogRotate(){
      _postfix=$(date -d '1 day ago' +'%Y%m%d')
      mv ${NGINX_LOG_DIR}/access.log ${NGINX_LOG_DIR}/access_${_postfix}.log
      mv ${NGINX_LOG_DIR}/error.log ${NGINX_LOG_DIR}/error_${_postfix}.log
     
      echo `date '+%Y-%m-%d %H:%M:%S'`'- 已归档当前日志文件，归档日期为xx_'${_postfix}>>${outputLogFile}
    } 
    
    # reload nginx worker，平滑切换
    executeReloadOutput(){
      echo `date '+%Y-%m-%d %H:%M:%S'`'- 归档执行结果：'$1>>${outputLogFile}
    
      if [ $1 -eq 0 ]; then
      # sh ${NGINX_PATH}/bin/ctrl.sh reload
       kill -USR1 `cat ${NGINX_PID_PATH}`
       echo `date '+%Y-%m-%d %H:%M:%S'`'- 已刷新nginx，输出到新日志文件流！'>>${outputLogFile}
      else
       echo `date '+%Y-%m-%d %H:%M:%S'`'- reload nginx 操作失败！'>>${outputLogFile}
      fi
    }
    
    # 清除历史归档文件
    executeClean(){
     # 当前日志文件夹大小
     current_dir_size=`du -b ${NGINX_LOG_DIR}|awk '{sum += $1};END {print int(sum/1024/1024/1024)}'`
    
     echo `date '+%Y-%m-%d %H:%M:%S'`'- 当前日志目录大小为：'$current_dir_size'G'>>${outputLogFile}
     
     if [ $current_dir_size -gt 150 ]; then
       find ${NGINX_LOG_DIR} -name 'access_*' -type f -mtime +$1 -exec rm {} \;
       echo `date '+%Y-%m-%d %H:%M:%S'`'- 已清除超过归档日期'$1'的access文件'`find ${NGINX_LOG_DIR} -name 'access_*' -type f -mtime +$1 -exec ls {} \;`>>${
    outputLogFile}
       
       find ${NGINX_LOG_DIR} -name 'error_*' -type f -mtime +$1 -exec rm {} \;
       echo `date '+%Y-%m-%d %H:%M:%S'`'- 已清除超过归档日期'$1'的error文件'`find ${NGINX_LOG_DIR} -name 'error_*' -type f -mtime +$1 -exec ls {} \;`>>${ou
    tputLogFile}
     fi
     
    }
    
    
    # 启动脚本程序
    start 30










> ### 统计脚本  

- 日志  
    
    
    |dea9cf09e1c84de52d547cf54c0301ee|[25/Dec/2018:18:35:22 +0800]|80.82.77.33|101.124.26.191|GET|GET / HTTP/1.1|259|500|-|text/html|259|2.021|-|-|Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.137 Safari/537.36|



- 统计IP访问量及所有调用量  
    
    
    awk '{print $0}' access.log | sort -n | uniq | wc -l  所有ip
    awk '{print $1}' access.log|sort |uniq |wc -l  
    
- 每秒客户端请求数 TOP5
    
    
    awk  -F'[ []' '{print $5}' access.log|sort|uniq -c|sort -rn|head -5
 
- 访问最频繁IP Top5  
    
    
    awk  -F'[ []' '{print $5}' access.log|sort|uniq -c|sort -rn|head -5
    
- 查看某一时段的IP访问量 
    
    
    如：四点到五点之间，流量暴增
    grep "25/Dec/2018:0[0-24]" access.log | awk '{print $1}' | sort | uniq -c| sort -nr | wc -l
    
- 查看访问最为频繁的100个ip
    
    
    awk '{print $1}' access.log | sort -n |uniq -c | sort -rn | head -n 100

- 查看访问100次以上的IP
    
    
    awk '{print $1}' access.log | sort -n |uniq -c |awk '{if($1 >100) print $0}'|sort -rn
    
- 查询某个IP的详细访问情况,按访问频率排序
    
    
    grep '127.0.01' access.log |awk '{print $7}'|sort |uniq -c |sort -rn |head -n 10


- 访问最频繁IP Top5  
    
    
    awk ‘{print $1}‘ access.log|sort |uniq -c | sort -rn |head -5


> ### 页面访问统计  


- 查看访问最频的页面(TOP100)  
    
    
    awk '{print $7}' access.log | sort |uniq -c | sort -rn | head -n 100

- 查看访问最频的页面(排除php页面且TOP100)  
   

    grep -v ".php"  access.log | awk '{print $7}' | sort |uniq -c | sort -rn | head -n 100

- 查看页面访问次数超过100次的页面  
    
    
    cat access.log | cut -d ' ' -f 7 | sort |uniq -c | awk '{if ($1 > 100) print $0}' | less

- 查看最近1000条记录，访问量最高的页面


    tail -1000 access.log |awk '{print $7}'|sort|uniq -c|sort -nr|less
    
- 统计每秒的请求数,top100的时间点(精确到秒)


    awk '{print $4}' access.log |cut -c 14-21|sort|uniq -c|sort -nr|head -n 100
   
    awk '{print $1}' access.log |wc -l
    

> ####根据IP统计访问UV

- 统计每分钟的请求数,top100的时间点(精确到分钟)  


    awk '{print $4}' access.log |cut -c 14-18|sort|uniq -c|sort -nr|head -n 100
    
- 统计每小时的请求数,top100的时间点(精确到小时) 


    awk '{print $4}' access.log |cut -c 14-15|sort|uniq -c|sort -nr|head -n 100


> ####性能分析 （在nginx log中最后一个字段加入$request_time）

- 列出传输时间超过 3 秒的页面，显示前20条  


    cat access.log|awk '($NF > 3){print $7}'|sort -n|uniq -c|sort -nr|head -20
    
- 列出php页面请求时间超过3秒的页面，并统计其出现的次数，显示前100条  


    cat access.log|awk '($NF > 1 &&  $7~/\.php/){print $7}'|sort -n|uniq -c|sort -nr|head -100
    
> ####蜘蛛抓取统计
- 统计蜘蛛抓取次数  


    grep 'Baiduspider' access.log |wc -l
    
- 统计蜘蛛抓取404的次数  


    grep 'Baiduspider' access.log |grep '404' | wc -l

> ####TCP连接统计

- 查看当前TCP连接数  


    netstat -tan | grep "ESTABLISHED" | grep ":80" | wc -l
    
- 用tcpdump嗅探80端口的访问看看谁最高  


    tcpdump -i eth0 -tnn dst port 80 -c 1000 | awk -F"." '{print $1"."$2"."$3"."$4}' | sort | uniq -c | sort -nr

> #### 实时统计  QPS  
    
    
    1 tail -f access.log | awk -F '[' '{print $2}' | awk 'BEGIN{key="";count=0}{if(key==$1){count++}else{printf("%s\t%d\r\n", key, count);count=1;key=$1}}'
    2 tail -f access.log | awk -F '[' '{print $2}' | awk '{print $1}' | uniq -c


> #### 非实时统计 QPS   
    
    cat access.log | awk -F '[' '{print $2}' | awk '{print $1}' | sort | uniq -c |sort -k1,1nr


> #### 截取 某秒的 数量   
    
    cat log.log |grep "压测日志"|awk -F  '|' '{print $2}' | cut -d  ':' -f 2 |sort -n| uniq -c
    



> #### 通过对日志格式的定义，就可以使用常见的 Linux 命令行工具进行分析了：  

- 查找访问频率最高的 URL 和次数：  
    
    
    cat access.log | awk -F '^A' '{print $10}' | sort | uniq -c

- 查找当前日志文件 500 错误的访问： 

    
    cat access.log | awk -F '^A '{if($5 == 500) print $0}'

- 查找当前日志文件 500 错误的数量：  
    
    
    cat access.log | awk -F '^A' '{if($5 == 500) print $0}' | wc -l

- 查找某一分钟内 500 错误访问的数量:  
 
    
    cat access.log | awk -F '^A' '{if($5 == 500) print $0}' | grep '09:00' | wc-l
    cat access.log | awk -F '^A' '{ print $0}' | grep '11:00' | wc -l
    
- 查找耗时超过 1s 的慢请求:  
    
    
    tail -f access.log | awk -F '^A' '{if($6>1) print $0}'

- 假如只想查看某些位:   
    
    
    tail -f access.log | awk -F "^A" "{if($6>1) print $3"|"$4}'

- 查找 502 错误最多的 URL：  
    
    
    cat access.log | awk -F '^A' '{if($5==502) print $11}' | sort | uniq -c

- 查找200空白页  
    
    
    cat access.log | awk -F '^A' '{if($5==200&&$8<100) print $3 "|" $4 "|" $11 "|" $6}'
    
- HTTP状态码(非200)统计 Top5  
   
    
    awk ‘{if ($13 != 200){print $13}}‘ access.log|sort|uniq -c|sort -rn|head -5

> ####分析请求数大于50000的源IP的行为  

    awk ‘{print $1}‘ access.log|sort |uniq -c |sort -rn|awk ‘{if ($1 > 50000){print $2}}‘ > tmp.txt
    for i in $(cat tmp.txt)
    do
       echo $i  >> analysis.txt
       echo "访问行为统计" >> analysis.txt
       grep $i  access.log|awk ‘{print $6}‘ |sort |uniq -c | sort -rn |head -5 >> analysis.txt
       echo "访问接口统计" >> analysis.txt
       grep $i  access.log|awk ‘{print $7}‘ |sort |uniq -c | sort -rn |head -5 >> analysis.txt
       echo -e "\n"  >> /root/analysis/$Ydate.txt
    done

    awk '{print $NF}' access.log|sort |uniq -c |sort -rn|awk '{if ($1 > 50000){print $2}}' > tmp.txt

> ####sql层面 统计规则
   
    https://blog.csdn.net/gamay/article/details/73166754

> ####统计访问量  
    
    
    **** 此句有问题***
    tail -f access.log | grep "cancel"|awk -F " |]"  '{print $2}' | cut -d ':' -f 3 | uniq -c
    tail access.log -f -s 10 --pid=16772|wc -l

> #### 统计访问URL统计PV  
    
    awk '{print $7}' access.log|wc -l

> #### 查询访问最频繁的URL  
    
    awk '{print $7}' access.log|sort | uniq -c |sort -n -k 1 -r|more



> #### 实用 命令行 
- 统计某一时刻处理请求的数量  
    
      
    wc -l access.log |awk '{print $1}'
    awk '{print $0}' access.log|grep "14:59" |wc -l
    awk '{print $0}' access.log|grep "10/Oct/2018" |wc -l
    awk '{print $0}' access.log|grep "14:59:00" |wc -l
    awk '{print $0}' /export/servers/logs/access.log |grep '24/Oct/2018' |wc -l
    awk '{print $0}' /export/servers/logs/access.log |grep '30/Oct/2018' |grep 200 |wc -l
    awk '{print $0}' /export/servers/logs/access.log |grep '23/Nov/2018:14' |grep 'miaojie-isv.isvjcloud.com' |wc
    /* 编码不为 200的  使用 '-v' 选项来反向匹配，选择不匹配的行  grep -E 'Dev.*Tech'  */
    awk '{print $0}' /export/servers/logs/access.log |grep '6/Nov/2018:13' |grep -v 200 |wc -l 
    awk '{print $0}' /export/servers/logs/access.log |grep '6/Nov/2018:13' |grep -v 200|404 |wc -l 
    awk '{print $0}' /export/servers/logs/access.log |grep '6/Nov/2018:13' |grep -e '20*|*02' |wc -l 
    awk '{print $0}' /export/servers/logs/access.log |grep '6/Nov/2018:13' |grep -E '200|502' |wc -l 

    >> refference:
    grep 做or 逻辑
    1.使用'|'
        如果你使用 'grep' 命令不带任何选项，你需要使用 '|' 来作OR条件的分隔符。
        grep 'pattern1\|pattern2' filename
    2.使用 -E
       ‘grep’ ‘-E’ 选项表示使用扩展的正则表达式。如果你使用 ‘grep’ 命令时带 ‘-E’，你只需要用途 ‘|’ 来分隔OR条件。
        grep -E 'pattern1|pattern2' filename
    3.使用egrep
        'egrep' 就是 'grep -E'。所以
        $ egrep 'Tech|Sales' employee.txt 100 Thomas Manager Sales $5,000 200 Jason Developer Technology $5,500 300 Raj Sysadmin Technology $7,000 500 Randy Manager Sales $6,000

    4.使用grep -e
        使用 ‘grep -e’ 选项你仅可以传递一个参数。在一个命令中使用多个 ‘-e’ 选项可以指定多个OR条件。Example:
        $ grep -e Tech -e Sales employee.txt 100 Thomas Manager Sales $5,000 200 Jason Developer Technology $5,500 300 Raj Sysadmin Technology $7,000 500 Randy Manager Sales $6,000
      
    >> grep and 
    grep and 没有命令
    ‘grep’ AND 使用 ‘-E pattern1.*pattern2’
    grep -E 'pattern1.*pattern2' filename grep -E 'pattern1.*pattern2|pattern2.*pattern1' filename
    grep -E 'Dev.*Tech' employee.txt 200 Jason Developer Technology $5,500
    
    ‘grep’ AND 使用多个 ‘grep’ 命令
    你还可以使用多个 ‘grep’ 命令加管道(pipe)的方式来实现 AND 操作
     grep -E 'pattern1' filename | grep -E 'pattern2'
     grep Manager employee.txt | grep Sales 100 Thomas Manager Sales $5,000 500 Randy Manager Sales $6,000
     
     >>   ‘grep’ NOT
     使用 ‘grep -v’ 你可以模拟NOT操作
     使用 ‘-v’ 选项来反向匹配，选择不匹配的行。
         grep -v 'pattern1' filename
     
     
    ##调用量大于1万的 筛选出
    
    查找7月17日访问log导出到17.log文件中：
    cat gelin_web_access.log | egrep "17/Jul/2017" | sed  -n '/00:00:00/,/23:59:59/p' > /tmp/17.log
    查看访问量前10的IP
    awk '{print $1}' 17.log | sort | uniq -c | sort -nr | head -n 10 
    查看访问前10的URL
    awk '{print $11}' gelin_web_access.log | sort | uniq -c | sort -nr | head -n 10
    查询访问最频繁的URL
    awk '{print $7}' gelin_web_access.log | sort | uniq -c | sort -n -k 1 -r | more
    查询访问最频繁的IP
    awk '{print $1}' gelin_web_access.log | sort | uniq -c | sort -n -k 1 -r | more
    根据访问IP统计UV
    awk '{print $1}' gelin_web_access.log | sort | uniq -c | wc -l
    统计访问URL统计PV
    awk '{print $7}' access.log | wc -l
    awk '{print $7}' access.log|grep "0"  | wc -l
    根据时间段统计查看日志
    cat gelin_web_access.log | sed -n '/17\/Jul\/2017:12/,/17\/Jul\/2017:13/p' | more
    
    ###并发数统计###
    连接数处理 
   
    


    响应大于10s的URL TOP10
    awk '{if ($12 > 10){print $7}}' access.log|sort|uniq -c|sort -rn |head -5


> nginx访问量统计

- 1.根据访问IP统计UV
    
    
    awk '{print $1}'  access.log|sort | uniq -c |wc -l

- 2.统计访问URL统计PV  
    
    
    awk '{print $7}' access.log|wc -l

- 3.查询访问最频繁的URL  
    
    
    awk '{print $7}' access.log|sort | uniq -c |sort -n -k 1 -r|more
    
- 4.查询访问最频繁的IP
    
    
    awk '{print $1}' access.log|sort | uniq -c |sort -n -k 1 -r|more

- 5.根据时间段统计查看日志  
    
    
    cat  access.log| sed -n '/14\/Mar\/2015:21/,/14\/Mar\/2015:22/p'|more
 
 
> 在nginx中，server_name 可以设置为ip地址吗？



> #### 网络 路线 

- IP网络层路由跟踪   
   
   
    host,dig,nslookup

-  传输层情况查看   
     
    
    netstat
    
-  列出所有端口 (包括监听和未监听的)   
   
     
    netstat -a
    netstat -a | more

-  列出所有 tcp 端口 
    
    
    netstat -at

-  查看Web服务器（Nginx Apache）的并发请求数及其TCP连接状态：  


        netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
        netstat -n | awk '/^tcp/ {++state[$NF]} END {for(key in state) print key,"t",state[key]}'
        结果参数描述

        CLOSED：无连接是活动的或正在进行
        LISTEN：服务器在等待进入呼叫
        SYN_RECV：一个连接请求已经到达，等待确认
        SYN_SENT：应用已经开始，打开一个连接
        ESTABLISHED：正常数据传输状态
        FIN_WAIT1：应用说它已经完成
        FIN_WAIT2：另一边已同意释放
        ITMED_WAIT：等待所有分组死掉
        CLOSING：两边同时尝试关闭
        TIME_WAIT：另一边已初始化一个释放
        LAST_ACK：等待所有分组死掉

        常用的三个状态是：ESTABLISHED 表示正在通信，TIME_WAIT 表示主动关闭，CLOSE_WAIT 表示被动关闭。


        TCP协议规定，对于已经建立的连接，网络双方要进行四次握手才能成功断开连接，如果缺少了其中某个步骤，将会使连接处于假死状态，连接本身占用的资源不会被释放。网络服务器程序要同时管理大量连接，所以很有必要保证无用连接完全断开，否则大量僵死的连接会浪费许多服务器资源。在众多TCP状态中，最值得注意的状态有两个：CLOSE_WAIT和TIME_WAIT。

        TIME_WAIT： TIME_WAIT 是主动关闭链接时形成的，等待2MSL时间，约4分钟。主要是防止最后一个ACK丢失。  由于TIME_WAIT 的时间会非常长，因此server端应尽量减少主动关闭连接
        CLOSE_WAIT： CLOSE_WAIT是被动关闭连接是形成的。根据TCP状态机，服务器端收到客户端发送的FIN，则按照TCP实现发送ACK，因此进入CLOSE_WAIT状态。但如果服务器端不执行close()，就不能由CLOSE_WAIT迁移到LAST_ACK，则系统中会存在很多CLOSE_WAIT状态的连接。此时，可能是系统忙于处理读、写操作，而未将已收到FIN的连接，进行close。此时，recv/read已收到FIN的连接socket，会返回0。为什么需要 TIME_WAIT 状态？
        为什么需要 TIME_WAIT 状态?
        假设最终的ACK丢失，server将重发FIN，client必须维护TCP状态信息以便可以重发最终的ACK，否则会发送RST，结果server认为发生错误。TCP实现必须可靠地终止连接的两个方向(全双工关闭)，client必须进入 TIME_WAIT 状态，因为client可能面 临重发最终ACK的情形。
        为什么 TIME_WAIT 状态需要保持 2MSL 这么长的时间
        如果 TIME_WAIT 状态保持时间不足够长(比如小于2MSL)，第一个连接就正常终止了。第二个拥有相同相关五元组的连接出现，而第一个连接的重复报文到达，干扰了第二个连接。TCP实现必须防止某个连接的重复报文在连接终止后出现，所以让TIME_WAIT状态保持时间足够长(2MSL)，连接相应方向上的TCP报文要么完全响应完毕，要么被 丢弃。建立第二个连接的时候，不会混淆。


        TIME_WAIT 和CLOSE_WAIT状态socket过多

        如果服务器出了异常，百分之八九十都是下面两种情况：
        1.服务器保持了大量TIME_WAIT状态
        2.服务器保持了大量CLOSE_WAIT状态，简单来说CLOSE_WAIT数目过大是由于被动关闭连接处理不当导致的。

        因为linux分配给一个用户的文件句柄是有限的，而TIME_WAIT和CLOSE_WAIT两种状态如果一直被保持，那么意味着对应数目的通道就一直被占着，而且是“占着茅坑不使劲”，一旦达到句柄数上限，新的请求就无法被处理了，接着就是大量Too Many Open Files异常，Tomcat崩溃。

        ------ 比较而言，我更喜欢用setsid，简单实用。当然，这里看大家喜好即可，效果上差别不大。-----
        https://blog.csdn.net/cy_cai/article/details/48158397






> ####列出所有处于监听状态的 Sockets   
     
     netstat -l
     只列出所有监听 tcp 端口 netstat -lt
     只列出所有监听 udp 端口 netstat -lu
     只列出所有监听 UNIX 端口 netstat -lx
 
> #### 显示每个协议的统计信息  
    
    netstat -s
    在 netstat 输出中显示 PID 和进程名称 netstat -p
    netstat -pt
    在 netstat 输出中不显示主机，端口和用户名 (host, port or user)
    netstat -an
    如果只是不想让这三个名称中的一个被显示，使用以下命令
    # netsat -a --numeric-ports
    # netsat -a --numeric-hosts
    # netsat -a --numeric-users
    持续输出 netstat 信息
    netstat -c
    netstat --verbose
    显示核心路由信息 
    netstat -r
    找出程序运行的端口
    netstat -an | grep ':80'
    netstat -ap | grep ssh
 
> #### 显示网络接口列表  
    
    netstat -i
    netstat -ie
 
 

   
 > #### 查看服务器的连接数/有效连接数  
     
    $ netstat -na|wc -l  #连接数
    $ netstat -nat|grep ESTABLISHED|wc -l  #有效连接数,中间的参数 ESTABLISHED表示有效的连接数！
    $ netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'    #对各种状态的连接数分组统计结果
 
> ####查看连接某服务端口最多的的IP地址  
    
    wss8848@ubuntu:~$ netstat -nat | grep "192.168.1.15:22" |awk '{print $5}'|awk -F: '{print $1}'|sort|uniq -c|sort -nr|head -20
    18 221.136.168.36
    3 154.74.45.242
    2 78.173.31.236
    2 62.183.207.98
    2 192.168.1.14
    2 182.48.111.215
    2 124.193.219.34
    2 119.145.41.2
    2 114.255.41.30
    1 75.102.11.99
 
 > ####TCP各种状态列表，各种状态的连接数  
     
     
    wss8848@ubuntu:~$ netstat -nat |awk '{print $6}'
    established)
    Foreign
    LISTEN
    TIME_WAIT
    ESTABLISHED
    TIME_WAIT
    SYN_SENT
 
 > ####先把状态全都取出来,然后使用uniq -c统计，之后再进行排序  
     
    wss8848@ubuntu:~$ netstat -nat |awk '{print $6}'|sort|uniq -c #或者
    wss8848@ubuntu:~$ netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}
 
    143 ESTABLISHED
    1 FIN_WAIT1
    1 Foreign
    1 LAST_ACK #正在等待处理的请求数
    36 LISTEN
    6 SYN_SENT
    113 TIME_WAIT #处理完毕，等待超时结束的请求数
    1 established)
    awk '{print $1}' access.log |sort|uniq -c|sort -nr|head -10
 
 
